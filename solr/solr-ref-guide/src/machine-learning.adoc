= Machine Learning
// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.


This section of the math expressions user guide covers machine learning
functions.

<<Feature Scaling, Feature Scaling>> -
<<Distance and Distance Measures, Distance>> -
<<knnSearch, knnSearch>> -
<<K-Nearest Neighbor (KNN), KNN>> -
<<K-Nearest Neighbor Regression, KNN Regression>> -
<<K-Means Clustering, K-means Clustering>> -
<<Fuzzy K-Means Clustering, Fuzzy K-means>>

== Feature Scaling

Before performing machine learning operations its often necessary to
scale the feature vectors so they can be compared at the same scale.

All the scaling functions below operate on vectors and matrices.
When operating on a matrix the rows of the matrix are scaled.

=== Min/Max Scaling

The `minMaxScale` function scales a vector or matrix between a minimum and maximum value.
By default it will scale between 0 and 1 if min/max values are not provided.

Below is a plot of a sine wave, with an amplitude of 1, before and
after it has been scaled between -5 and 5.

image::images/math-expressions/minmaxscale.png[]


Below is a simple example of min/max scaling of a matrix between 0 and 1.
Notice that once brought into the same scale the vectors are the same.

[source,text]
----
let(a=array(20, 30, 40, 50),
    b=array(200, 300, 400, 500),
    c=matrix(a, b),
    d=minMaxScale(c))
----

When this expression is sent to the `/stream` handler it responds with:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "d": [
          [
            0,
            0.3333333333333333,
            0.6666666666666666,
            1
          ],
          [
            0,
            0.3333333333333333,
            0.6666666666666666,
            1
          ]
        ]
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 0
      }
    ]
  }
}
----

=== Standardization

The `standardize` function scales a vector so that it has a
mean of 0 and a standard deviation of 1.

Below is a plot of a sine wave, with an amplitude of 1, before and
after it has been standardized.

image::images/math-expressions/standardize.png[]

Below is a simple example of of a standardized matrix.
Notice that once brought into the same scale the vectors are the same.

[source,text]
----
let(a=array(20, 30, 40, 50),
    b=array(200, 300, 400, 500),
    c=matrix(a, b),
    d=standardize(c))
----

When this expression is sent to the `/stream` handler it responds with:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "d": [
          [
            -1.161895003862225,
            -0.3872983346207417,
            0.3872983346207417,
            1.161895003862225
          ],
          [
            -1.1618950038622249,
            -0.38729833462074165,
            0.38729833462074165,
            1.1618950038622249
          ]
        ]
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 17
      }
    ]
  }
}
----

=== Unit Vectors

The `unitize` function scales vectors to a magnitude of 1. A vector with a
magnitude of 1 is known as a unit vector. Unit vectors are preferred
when the vector math deals with vector direction rather than magnitude.

Below is a plot of a sine wave, with an amplitude of 1, before and
after it has been unitized.

image::images/math-expressions/unitize.png[]

Below is a simple example of a unitized matrix.
Notice that once brought into the same scale the vectors are the same.

[source,text]
----
let(a=array(20, 30, 40, 50),
    b=array(200, 300, 400, 500),
    c=matrix(a, b),
    d=unitize(c))
----

When this expression is sent to the `/stream` handler it responds with:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "d": [
          [
            0.2721655269759087,
            0.40824829046386296,
            0.5443310539518174,
            0.6804138174397716
          ],
          [
            0.2721655269759087,
            0.4082482904638631,
            0.5443310539518174,
            0.6804138174397717
          ]
        ]
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 6
      }
    ]
  }
}
----

== Distance and Distance Measures

The `distance` function computes the distance for two numeric arrays or a distance matrix for the columns of a matrix.

There are five distance measure functions that return a function that performs the actual distance calculation:

* `euclidean` (default)
* `manhattan`
* `canberra`
* `earthMovers`
* `cosine`
* `haversineMeters` (Geospatial distance measure)

The distance measure functions can be used with all machine learning functions
that support distance measures.

Below is an example for computing Euclidean distance for two numeric arrays:

[source,text]
----
let(a=array(20, 30, 40, 50),
    b=array(21, 29, 41, 49),
    c=distance(a, b))
----

When this expression is sent to the `/stream` handler it responds with:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "c": 2
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 0
      }
    ]
  }
}
----

Below the distance is calculated using *Manahattan* distance.

[source,text]
----
let(a=array(20, 30, 40, 50),
    b=array(21, 29, 41, 49),
    c=distance(a, b, manhattan()))
----

When this expression is sent to the `/stream` handler it responds with:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "c": 4
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 1
      }
    ]
  }
}
----


Below is an example for computing a distance matrix for columns
of a matrix:

[source,text]
----
let(a=array(20, 30, 40),
    b=array(21, 29, 41),
    c=array(31, 40, 50),
    d=matrix(a, b, c),
    c=distance(d))
----

When this expression is sent to the `/stream` handler it responds with:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "e": [
          [
            0,
            15.652475842498529,
            34.07345007480164
          ],
          [
            15.652475842498529,
            0,
            18.547236990991408
          ],
          [
            34.07345007480164,
            18.547236990991408,
            0
          ]
        ]
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 24
      }
    ]
  }
}
----

== knnSearch

The `knnSearch` function returns the k-nearest neighbors
for a document based on text similarity.
Under the covers the `knnSearch` function
uses the More Like This query parser plugin. This capability uses the search
engines query, term statistics, scoring and ranking capability to perform a fast,
nearest neighbor search for similar documents over large distributed indexes.

The results of this
search can be used directly or provide *candidates* for machine learning operations such
as a secondary knn vector search.

The example below shows the `knnSearch` function run over a movie reviews data set. The
search returns the 50 documents most similar to document id *83e9b5b0-...* based on
the similarity of the *review_t* field which contains
the text of the review. The *mindf* and *maxdf* specify the min and max document frequency of the terms
used to perform the search. This makes the query faster by eliminating very high frequency terms
and terms and also improves accuracy be removing noise from search.


image::images/math-expressions/knnSearch.png[]

NOTE: In this example the `select`
function is used to truncate the review in the output to 220 characters to make it easier
to read in a table.


== K-Nearest Neighbor (KNN)

The `knn` function searches the rows of a matrix with a search vector and
returns a matrix of the k-nearest neighbors. This allows for secondary vector
searches over result sets.

The `knn` function supports changing of the distance measure by providing one of the following
distance measure functions:

* `euclidean` (Default)
* `manhattan`
* `canberra`
* `earthMovers`
* `cosine`
* `haversineMeters` (Geospatial distance measure)

The example below shows how to perform a secondary search over an aggregation
result set. The goal of the example is to find zip codes in the nyc311 complaint
database that have similar complaint types to the zip code 10280.

The first step in the example is to use the `facet2D` function to perform a two
dimensional aggregation over the *zip_s* and *complaint_type* fields. In the example
the top 119 zip codes and top 5 complaint types for each zip code are calculated
for the borough of Manhattan. The result is a list of tuples each containing
the *zip_s*, *complaint_type_s* and the *count* for combination.

The list of tuples is then *pivoted* into a matrix with the `pivot` function. The
`pivot` function in this example returns a matrix with rows of zip codes
and columns of complaint types. The `count(*)` field from the tuples
populates the cells of the matrix. This matrix will be used as the secondary
search matrix.

The next step is to locate the vector for the 10280 zip code. This is done in
three steps in the example. The first step is to retrieve the row labels from
the matrix with `getRowLabels` function. The row labels in this case are zip codes which were populated
by the `pivot` function. Then the `indexOf` function is used
to find the *index* of the "10280" zip code in the list of row labels. The `rowAt`
function is then used to return the vector at that *index* from the matrix. This vector
is the *search vector*.

Now that we have a matrix and search vector we can use the `knn` function to perform the search.
In the example the `knn` function searches the matrix with the search vector with a K of 5, using
*cosine* distance. Cosine distance is useful for comparing sparse vectors which is the case in this
example. The `knn` function returns a matrix with the top 5 nearest neighbors to the search vector.

The `knn` function populates the row and column labels of the return matrix and
also adds a vector of *distances* for each row as an attribute to the matrix.

In the example the `zplot` function extracts the row labels and
the distance vector with the `getRowLabels` and `getAttribute` functions.
The `topFeatures` function is used to extract
the top 5 column labels for each zip code vector, based on the counts for each
column. Then `zplot` outputs the data in a format that can be visualized in
a table with Zeppelin-Solr.

image::images/math-expressions/knn.png[]

The table above shows each zip code returned by the `knn` function along
with the list of complaints and distances. These are the zip codes that are most similar
to the 10280 zip code based on complaint types in Manhattan.

== K-Nearest Neighbor Regression

K-nearest neighbor regression is a non-linear, bivariate and multivariate regression method.
Knn regression is a lazy learning
technique which means it does not fit a model to the training set in advance. Instead the
entire training set of observations and outcomes are held in memory and predictions are made
by averaging the outcomes of the k-nearest neighbors.

The `knnRegress` function is used to perform nearest neighbor regression.


=== 2D Non-Linear Regression

The example below shows the *regression plot* for knn regression applied to a 2D scatter plot.

In this example the `random` function is used to draw 500 random samples from the *logs* collection
containing two fields *filesize_d* and *eresponse_d*. The sample is then vectorized with the
*filesize_d* field stored in a vector assigned to variable *x* and the *eresponse_d* vector stored in
variable *y*. The `knnRegress` function is then applied with 20 as the nearest neighbor parameter,
which returns a knn function which can be used to predict values.
The `predict` function is then called on the knn function to predict values for the original *x* vector.
Finally `zplot` is used to plot original *x* and *y* vectors along with the predictions.

image::images/math-expressions/knnRegress.png[]

Notice that the regression plot shows a non-linear relations ship between the *filesize_d*
field and the *eresponse_d* field. Also note that knn regression
plots a non-linear curve through the scatter plot. The larger the size
of K (nearest neighbors), the smoother the line.

=== Multivariate Non-Linear Regression

The `knnRegress` function prepares the training set for use with the `predict` function.

Below is an example of the `knnRegress` function. In this example 10,000 random samples
are taken, each containing the variables `filesize_d`, `service_d` and `response_d`. The pairs of
`filesize_d` and `service_d` will be used to predict the value of `response_d`.


[source,text]
----
let(a=random(logs, q="*:*", rows="500", fl="filesize_d,  load_d, eresponse_d"),
     x=col(a, filesize_d),
     y=col(a, load_d),
     z=col(a, eresponse_d),
     obs=transpose(matrix(x, y)),
     r=knnRegress(obs, z , 20))
----

This expression returns the following response. Notice that `knnRegress` returns a tuple describing the regression inputs:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "lazyModel": {
          "features": 2,
          "robust": false,
          "distance": "EuclideanDistance",
          "observations": 10000,
          "scale": false,
          "k": 5
        }
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 170
      }
    ]
  }
}
----

=== Prediction and Residuals

The output of `knnRegress` can be used with the `predict` function like other regression models.

In the example below the `predict` function is used to predict results for the original training
data. The sumSq of the residuals is then calculated.

[source,text]
----
let(samples=random(collection1, q="*:*", rows="10000", fl="filesize_d, service_d, response_d"),
    filesizes=col(samples, filesize_d),
    serviceLevels=col(samples, service_d),
    outcomes=col(samples, response_d),
    observations=transpose(matrix(filesizes, serviceLevels)),
    lazyModel=knnRegress(observations, outcomes , 5),
    predictions=predict(lazyModel, observations),
    residuals=ebeSubtract(outcomes, predictions),
    sumSqErr=sumSq(residuals))
----

This expression returns the following response:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "sumSqErr": 1920290.1204126712
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 3796
      }
    ]
  }
}
----

=== Setting Feature Scaling

If the features in the observation matrix are not in the same scale then the larger features
will carry more weight in the distance calculation then the smaller features. This can greatly
impact the accuracy of the prediction. The `knnRegress` function has a `scale` parameter which
can be set to `true` to automatically scale the features in the same range.

The example below shows `knnRegress` with feature scaling turned on.

Notice that when feature scaling is turned on the `sumSqErr` in the output is much lower.
This shows how much more accurate the predictions are when feature scaling is turned on in
this particular example. This is because the `filesize_d` feature is significantly larger then
the `service_d` feature.

[source,text]
----
let(samples=random(collection1, q="*:*", rows="10000", fl="filesize_d, service_d, response_d"),
    filesizes=col(samples, filesize_d),
    serviceLevels=col(samples, service_d),
    outcomes=col(samples, response_d),
    observations=transpose(matrix(filesizes, serviceLevels)),
    lazyModel=knnRegress(observations, outcomes , 5, scale=true),
    predictions=predict(lazyModel, observations),
    residuals=ebeSubtract(outcomes, predictions),
    sumSqErr=sumSq(residuals))
----

This expression returns the following response:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "sumSqErr": 4076.794951120683
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 3790
      }
    ]
  }
}
----


=== Setting Robust Regression

The default prediction approach is to take the mean of the outcomes of the k-nearest
neighbors. If the outcomes contain outliers the mean value can be skewed. Setting
the `robust` parameter to `true` will take the median outcome of the k-nearest neighbors.
This provides a regression prediction that is robust to outliers.

=== Setting the Distance Measure

The distance measure can be changed for the k-nearest neighbor search by adding a distance measure
function to the `knnRegress` parameters. Below is an example using `manhattan` distance.

[source,text]
----
let(samples=random(collection1, q="*:*", rows="10000", fl="filesize_d, service_d, response_d"),
    filesizes=col(samples, filesize_d),
    serviceLevels=col(samples, service_d),
    outcomes=col(samples, response_d),
    observations=transpose(matrix(filesizes, serviceLevels)),
    lazyModel=knnRegress(observations, outcomes, 5, manhattan(), scale=true),
    predictions=predict(lazyModel, observations),
    residuals=ebeSubtract(outcomes, predictions),
    sumSqErr=sumSq(residuals))
----

This expression returns the following response:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "sumSqErr": 4761.221942288098
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 3571
      }
    ]
  }
}
----


== K-Means Clustering

The `kmeans` functions performs k-means clustering of the rows of a matrix.
Once the clustering has been completed there are a number of useful functions available
for examining the clusters and centroids.


=== 2D Cluster Visualization

The `zplot` function has direct support for plotting 2D clusters by using the *clusters* named parameter.
The example below demonstrates this capability by clustering and visualizing latitude and longitude points.

==== Clustered Scatter Plot

In this example the `random` function draws a sample of records from the nyc311 (complaints database) collection where
the complaint description matches "rat sighting" and latitude is populated in the record. The latitude and longitude fields
are then vectorized and added as rows to a matrix. The matrix is transposed so each row contains a single latitude, longitude
point. The `kmeans` function is then used to cluster the latitude and longitude points into 21 clusters.
Finally, the `zplot` function
is used to visualize the clusters as a scatter plot.

image::images/math-expressions/2DCluster1.png[]

The scatter plot above shows each lat/lon point plotted on a Euclidean plain with longitude on the
*x* axis and
latitude on *y* axis. Each cluster is shown in a different color. This plot provides interesting
insight into the clusters of rat sightings throughout the five boroughs of New York City. For
example it highlights a cluster of dense sightings in Brooklyn at cluster5 and cluster17,
surrounded by less dense clusters.

==== Plotting the Centroids

The centroids of each cluster can then be easily plotted on a *map* to visualize the center of the
clusters. In the example below the centroids are extracted from the clusters using the `getCentroids`
function, which returns a matrix of the centroids.

The centroids matrix contains 2D lan/lon points. The `colAt` function can then be used
to extract the latitude and longitude columns by index from the matrix so they can be
plotted with `zplot`. A map visualization is used below to display the centroids.


image::images/math-expressions/centroidplot.png[]


The map can then be zoomed to get a closer look at the centroids in the high density areas shown
in the cluster scatter plot.

image::images/math-expressions/centroidzoom.png[]



=== Phrase Extraction

Clustering can also be used to extract key phrases from a text field in a search result set. The example below
demonstrates this capability.

NOTE: The example below works with TF-IDF _term vectors_.
The section <<term-vectors.adoc#term-vectors,Text Analysis and Term Vectors>> offers
a full explanation of this features.


In the example the `search` function returns documents where the *review_t* field matches the phrase "star wars".
The `select` function is run over the result set and applies the `analyze` function
which uses the Lucene/Solr analyzer attached to the schema field *text_bigrams* to re-analyze the *review_t*
field. This analyzer returns bigrams which are then annotated to documents in a field called *terms*.

The `termVectors` function then creates TD-IDF term vectors from the bigrams stored in the *terms* field.
The `kmeans` function is then used to cluster the bigram term vectors.
Finally the top 5 features are extracted from the centroids an returned. Notice
that the features are all bigram phrases with semantic significance to the result set.


[source,text]
----
let(a=select(search(reviews, q="review_t:\"star wars\"", rows="500"),
                    id,
                    analyze(review_t, text_bigrams) as terms),
    vectors=termVectors(a, maxDocFreq=.10, minDocFreq=.03, minTermLength=13, exclude="_,br,have"),
    clusters=kmeans(vectors, 5),
    centroids=getCentroids(clusters),
    phrases=topFeatures(centroids, 5))
----

When this expression is sent to the `/stream` handler it responds with:

[source,text]
----
{
  "result-set": {
    "docs": [
      {
        "phrases": [
          [
            "empire strikes",
            "rebel alliance",
            "princess leia",
            "luke skywalker",
            "phantom menace"
          ],
          [
            "original star",
            "main characters",
            "production values",
            "anakin skywalker",
            "luke skywalker"
          ],
          [
            "carrie fisher",
            "original films",
            "harrison ford",
            "luke skywalker",
            "ian mcdiarmid"
          ],
          [
            "phantom menace",
            "original trilogy",
            "harrison ford",
            "john williams",
            "empire strikes"
          ],
          [
            "science fiction",
            "fiction films",
            "forbidden planet",
            "character development",
            "worth watching"
          ]
        ]
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 46
      }
    ]
  }
}
----



== Multi K-Means Clustering

K-means clustering will produce different results depending on
the initial placement of the centroids. K-means is also fast enough
that multiple trials can be performed and the best outcome selected.

The `multiKmeans` function runs the k-means clustering algorithm for a given number of trials and selects the
best result based on which trial produces the lowest intra-cluster variance.

The example below is identical to centroids example except that it uses `multiKmeans` with 100 trials,
rather than a single trial of the `kmeans` function.

[source,text]
----
let(a=select(search(reviews, q="review_t:\"star wars\"", rows="500"),
                    id,
                    analyze(review_t, text_bigrams) as terms),
    vectors=termVectors(a, maxDocFreq=.10, minDocFreq=.03, minTermLength=13, exclude="_,br,have"),
    clusters=multiKmeans(vectors, 5, 15),
    centroids=getCentroids(clusters),
    phrases=topFeatures(centroids, 5))
----

This expression returns the following response:

[source,json]
----
{
  "result-set": {
    "docs": [
      {
        "phrases": [
          [
            "science fiction",
            "original star",
            "production values",
            "fiction films",
            "forbidden planet"
          ],
          [
            "empire strikes",
            "princess leia",
            "luke skywalker",
            "phantom menace"
          ],
          [
            "carrie fisher",
            "harrison ford",
            "luke skywalker",
            "empire strikes",
            "original films"
          ],
          [
            "phantom menace",
            "original trilogy",
            "harrison ford",
            "character development",
            "john williams"
          ],
          [
            "rebel alliance",
            "empire strikes",
            "princess leia",
            "original trilogy",
            "luke skywalker"
          ]
        ]
      },
      {
        "EOF": true,
        "RESPONSE_TIME": 84
      }
    ]
  }
}
----

== Fuzzy K-Means Clustering

The `fuzzyKmeans` function is a soft clustering algorithm which
allows vectors to be assigned to more then one cluster. The `fuzziness` parameter
is a value between 1 and 2 that determines how fuzzy to make the cluster assignment.

After the clustering has been performed the `getMembershipMatrix` function can be called
on the clustering result to return a matrix describing the probabilities
of cluster membership for each vector.
This matrix can be used to understand relationships between clusters.

In the example below `fuzzyKmeans` is used to cluster the movie reviews matching the phrase "star wars".
But instead of looking at the clusters or centroids the `getMembershipMatrix` is used to return the
membership probabilities for each document. The membership matrix is comprised of a row for each
vector that was clustered. There is a column in the matrix for each cluster.
The values in the matrix contain the probability that a specific vector belongs to a specific cluster.

In the example the `distance` function is then used to create a *distance matrix* from the columns of the
membership matrix. The distance matrix is then visualized with the `zplot` function as a heat map. Notice
that the heat map has been configured to increase in color intensity as the distance shortens.

In the example cluster1 and cluster5 have the shortest distance between the clusters.
Further analysis of the features in both clusters can be performed to understand
the relationship between cluster1 and cluster5.

image::images/math-expressions/fuzzyk.png[]
